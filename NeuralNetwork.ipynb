{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempting a fully connected NN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#only for rnd numbers and calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a neuron class\n",
    "\n",
    "class neuron():\n",
    "\n",
    "    #instatiate neuron with random initial weight and bias, and number of inputs\n",
    "    def __init__(self, no_inputs, activation_fnc):\n",
    "        #initialise weights\n",
    "        self.inputs = no_inputs\n",
    "        self.weights = np.random.sample(no_inputs)\n",
    "        self.bias = np.random.randn()\n",
    "        self.activation = activation_fnc\n",
    "        \n",
    "        self.gradients = np.zeros(no_inputs)\n",
    "\n",
    "\n",
    "    def neural_function(self, inputs):\n",
    "        #y = sum(x*w) + b\n",
    "\n",
    "        #apply weights and bias to input list (must be ordered)\n",
    "        if len(inputs) == self.inputs:\n",
    "            neuron_output = sum(inputs*self.weights) + self.bias\n",
    "        else:\n",
    "            print(self.inputs, inputs)\n",
    "            print('invalid input length')\n",
    "\n",
    "        return neuron_output\n",
    "\n",
    "    def process(self, inputs):\n",
    "        #apply activation and neural processing to inputs\n",
    "        #main function of the neuron\n",
    "\n",
    "        output = self.activation(self.neural_function(inputs))\n",
    "\n",
    "        return output\n",
    "    \n",
    "    \n",
    "#     def gradFinder(self, output, expected_output, lastInput, loss_function = MSE_func):\n",
    "#         interval = 0.000001 \n",
    "\n",
    "#         #update weight with incrental change\n",
    "#         second_weights = [weight + interval for weight in self.weights]\n",
    "#         self.weights = second_weights\n",
    "\n",
    "#         #Update output of neuron with new weight\n",
    "#         second_out = self.FeedForward(lastInput)\n",
    "\n",
    "#         #find gradient in cost function based on this weight\n",
    "#         new_loss = loss_function(second_out, expected_output)\n",
    "#         gradient = (new_loss - old_loss)/interval #wrt weight\n",
    "    \n",
    "#         return gradient\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer():\n",
    "    #create a dense layer of neurons\n",
    "    #no neurons is output size of layer\n",
    "    \n",
    "    def __init__(self, input_size, no_neurons, activation_func):\n",
    "        self.input_size = input_size\n",
    "        self.layer_size = no_neurons\n",
    "        self.activation = activation_func\n",
    "        \n",
    "        #create the layer when instantiated (creates list of neurons)\n",
    "        self.neuron_layer = [neuron(self.input_size,self.activation) for i in range(self.layer_size)]\n",
    "        \n",
    "    \n",
    "    def FeedForward(self, input_data):\n",
    "        #passes data through each neuron in the layer for list of outputs\n",
    "        \n",
    "        prediction = [neuron.process(input_data) for neuron in self.neuron_layer]\n",
    "        return prediction\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a network class to handle network level operations\n",
    "\n",
    "class NeuralNet():\n",
    "    \n",
    "    def __init__(self, layers = []):\n",
    "        self.layers = layers\n",
    "        #stops you adding more layers everytime the network is instantiated\n",
    "        self.layers.clear()\n",
    "        \n",
    "        \n",
    "    def addLayer(self, input_size, no_neurons, activation_fnc):\n",
    "        newLayer = layer(input_size, no_neurons, activation_fnc)\n",
    "        (self.layers).append(newLayer)\n",
    "        \n",
    "        \n",
    "    def FeedForward(self, input_data):\n",
    "        \n",
    "        for i in range(len(self.layers)):\n",
    "            output = (self.layers[i]).FeedForward(input_data)\n",
    "            input_data = output\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    \n",
    "#     def Gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation function\n",
    "\n",
    "def ReLU_fnc(neuron_out):\n",
    "    #simple ReLU activation to allow description of non-linear functions\n",
    "    #output = 0 if x < 0; x if x > 0 {--/}\n",
    "\n",
    "    output = max(0.0, neuron_out)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function \n",
    "\n",
    "#(mean squared error implementation)\n",
    "def MSE_func(outputs, expected_outputs):\n",
    "    MSE = sum(np.square(np.subtract(expected_outputs, outputs)))/len(outputs)\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient finder\n",
    "\n",
    "#loss as a function of weight and bias\n",
    "#find gradient in weight axis for individual neuron\n",
    "def gradFinder(output, expected_output, neuron, lastInput, loss_function = MSE_func):\n",
    "    interval = 0.000001 \n",
    "    \n",
    "    #update weight with incrental change\n",
    "    second_weight = neuron.weight[0] + interval\n",
    "    neuron.weight = second_weight\n",
    "    \n",
    "    #Update output of neuron with new weight\n",
    "    second_out = neuron.FeedForward(lastInput)\n",
    "    \n",
    "    #find gradient in cost function based on this weight\n",
    "    new_loss = loss_function(second_out, expected_output)\n",
    "    gradient = (new_loss - old_loss)/interval #wrt weight\n",
    "    \n",
    "    return gradient\n",
    "    \n",
    "#     listofOutputs = [neuron.weights[0] = listOfweights_item, neuron for listOfweights_item in listOfweights]\n",
    "#     weightLossList = [loss_function(expected_output, layer4.FeedForward(output)) for output in listOfOutputs]\n",
    "#     Wgradient = np.grad(weightLossList)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimiser function\n",
    "\n",
    "#Gradient descent\n",
    "def GradDescent(gradient, learning_rate, neuron):\n",
    "    neuron.weight[0] = weight[0] - gradient*learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backpropagation (make it learn)\n",
    "#the hardest part!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple net example\n",
    "labels = [1,5,8,6]\n",
    "data = [[4,2,3,4],[8,6,7,4],[2,1,8,1], [9,6,3,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #define the neural net\n",
    "# layer1 = layer(4,8,ReLU_fnc)\n",
    "# layer2 = layer(8,8,ReLU_fnc)\n",
    "# layer3 = layer(8,4,ReLU_fnc)\n",
    "# layer4 = layer(4,1,ReLU_fnc)\n",
    "\n",
    "nn = NeuralNet()\n",
    "\n",
    "nn.addLayer(4,8,ReLU_fnc)\n",
    "nn.addLayer(8,8,ReLU_fnc)\n",
    "nn.addLayer(8,4,ReLU_fnc)\n",
    "nn.addLayer(4,1,ReLU_fnc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[242.66813778719492]\n"
     ]
    }
   ],
   "source": [
    "# #feed forward (make a prediction)\n",
    "# output1 = layer1.FeedForward(data[0])\n",
    "# output2 = layer2.FeedForward(output1)\n",
    "# output3 = layer3.FeedForward(output2)\n",
    "# output4 = layer4.FeedForward(output3)\n",
    "\n",
    "# print(output4)\n",
    "\n",
    "output = nn.FeedForward(data[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58403.48882153063\n"
     ]
    }
   ],
   "source": [
    "#calculate loss\n",
    "loss = MSE_func(output, labels[0])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #define the neural net\n",
    "# layer1 = [neuron(4,ReLU_fnc) for i in range(4)]\n",
    "# layer2 = [neuron(4,ReLU_fnc) for i in range(8)]\n",
    "# layer3 = [neuron(8,ReLU_fnc) for i in range(8)]\n",
    "# layer4 = [neuron(8,ReLU_fnc) for i in range(4)]\n",
    "# layer5 = [neuron(4,ReLU_fnc) for i in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #feed forward (make a prediction)\n",
    "\n",
    "# print(data[0])\n",
    "\n",
    "# output1 = [neuron.process(data[0]) for neuron in layer1]\n",
    "# output2 = [neuron.process(output1) for neuron in layer2]\n",
    "# output3 = [neuron.process(output2) for neuron in layer3]\n",
    "# output4 = [neuron.process(output3) for neuron in layer4]\n",
    "# output5 = [neuron.process(output4) for neuron in layer5]\n",
    "\n",
    "# print(output5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
